{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b593b-175f-4df3-bfdf-d2c746e5ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5883726-aaf7-4628-a4c3-7ee0d71614b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web.\n",
    "\n",
    "Python web scraping is an automated method used for collecting large amounts of data from websites and storing it in a structured form.\n",
    "\n",
    "Some of the main use cases of web scraping include price monitoring, price intelligence, news monitoring, lead generation, and market research among many others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f0e58-c39b-4266-813d-61bed9388a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910b6ea-4570-4cd5-9bb7-4f7a05aa08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML Parsing\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "DOM Parsing\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "\n",
    "Vertical Aggregation\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278539a7-59ec-4a2c-92b3-b3ab2e3121b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d72317-068a-4c0b-9db8-95f0cd70de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed web pages based on specific criteria that can be used to extract, navigate, search,\n",
    "and modify data from HTML, which is mostly used for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52b01d-f4a1-431a-b13f-e7128ef3ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Easy to Learn and Use:\n",
    "    Beautiful Soup is designed to be easy to learn and use, making it accessible to both beginners and experienced developers. It provides a simple interface for navigating and searching the parse tree.\n",
    "HTML and XML Parsing:\n",
    "    Beautiful Soup supports both HTML and XML parsing, making it versatile for extracting data from a variety of sources. It automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "Built-in Parsers:\n",
    "    Beautiful Soup supports different parsers, including Python’s built-in html.parser, lxml’s HTML parser, and xml parser. This flexibility allows users to choose the parser that best suits their needs.\n",
    "Tree Navigation:\n",
    "    Beautiful Soup provides methods for navigating the parse tree, such as finding tags, searching for specific elements, and navigating the tree's structure. This makes it easy to extract information from HTML or XML documents.\n",
    "Powerful Tag and Attribute Searching:\n",
    "    Beautiful Soup allows searching for tags and attributes using various methods, such as tag names, CSS classes, or attribute values. This makes it straightforward to locate and extract specific elements.\n",
    "Support for Regular Expressions:\n",
    "    Beautiful Soup allows the use of regular expressions for more complex and flexible searching, providing additional power for custom requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd5b5c-6e95-4843-bf9d-60a82a32766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53243f60-aa57-421e-a932-44aa2dbb17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flask provides a simple and consistent interface to the incoming HTTP request data. From accessing form data, file uploads, cookies, and headers to handling JSON data, \n",
    "Flask's request handling capabilities make it easy to build robust and secure web applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc319a9-6628-4c86-bc0f-8c9545427446",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb7445-60bd-4520-9dc4-4c3a705834f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Amazon EC2 (Elastic Cloud Compute)\n",
    "Amazon EC2 is the fastest cloud computing service provided by AWS. It offers virtual, secure, reliable, and resizable servers for any workload. Through this service, it becomes easy for developers to access resources and also facilitates web-scale cloud computing. This comes with the best suitable processors, networking facilities, and storage systems. Developers can quickly and dynamically scale capacities as per business needs. It has over 500 instances and you can also choose the latest processor, operating system, storage, and networking to help you choose according to the needs of the business. Also, with Amazon EC2, you only have to pay for what you use, and also as per the time period, scale with amazon EC2 auto-scaling has optimal storage and can optimize CPU configurations.\n",
    "\n",
    "2. Amazon RDS (Relational Database Services) \n",
    "Amazon RDS (Relational Database Service) is another service provided by AWS which is a managed database for PostgreSQL, MariaDB, MySQL, and Oracle. Using Amazon RDS, you can set up, operate, and scale databases in the cloud. It provides high performance by automating the tasks like database setup, hardware provisioning, patching, and backups. Also, it helps in cost optimization by providing high availability, compatibility, and security for resources, and there’s no need to install and manage the database software. during its usage. As per the need, you can easily choose any engine out of 15+ engines some of them being MySQL, PostgreSQL, Oracle, etc. It is a highly secure and easily available AWS service.\n",
    "\n",
    "3. Amazon S3 (Simple Storage Service)\n",
    "With Amazon, it has become easy to store data anytime, anywhere. Amazon S3 (Simple Storage Service), one of the best services provided by AWS is an object storage service offering scalability, availability, security, and high-performing. You can also retrieve data, data here is stored in “storage classes” where there’s no requirement of extra investment and you can also manage it well. Amazon S3 is the perfect fit for big businesses where a large amount of data is managed for varied purposes. It comes with handling any volume of data with its robust access controls, and replication tools prevent accidental deletion, and also maintains data version controls. \n",
    "\n",
    "4. Amazon IAM (Identity and Access Management)\n",
    "Amazon IAM (Identity and Access Management) allows users to securely access and manage resources. To achieve complete access to the tools and resources provided by AWS, AWS IAM is the best AWS service. It gives you the right to have control over who has authorization (signed in) and authentication (has permissions) access to the resources. It comes with attribute-based access control which helps you to create separate permissions on the basis of the user’s attributes such as job role, department, etc. Through this, you can allow or deny access given to users. AWS IAM has complete access or is a central manager for refining permissions across AWS. He/She handles who can access what.\n",
    "\n",
    "5. Amazon EBS (Elastic Block Store)\n",
    "Amazon EBS is the next service provided by AWS which is a block storage solution specifically designed for Amazon EC2. Throughout a workload of any size, Amazon EBS helps to securely manage transactions. You can handle diverse workloads, be it relational, non-relational, or business applications. You get to choose between five different volume types so as to achieve effectiveness and optimum cost. It helps to resize workloads for big data analytics engines such as Hadoop and Spark. Its lifecycle management creates policies to create and manage backups effectively. It supports high-performance scaling workloads such as Microsoft, and SAP products. \n",
    "\n",
    "6. Amazon Lambda\n",
    "Another promising service by AWS is Amazon Lambda which is a serverless and event-driven computing service that lets you run code for virtual applications or backend services automatically. You need to worry about servers and clusters when working with solutions using Amazon Lambda. It is also cost-effective where you have to only pay for the services you use. As a user, your responsibility is to just upload the code and Lambda handles the rest. Using Lambda, you get precise software scaling and extensive availability. With hundreds to thousands of workloads per second, AWS Lambda responsibly handles code execution requests. It is one of the best services provided by AWS for developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e75d89-5fa6-4380-9fe1-f3a1c8f357ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad55c4d-50a7-4611-9a5d-4da9cbc3f9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69362442-656d-436c-aa03-88ec9f30c32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1880a-42fd-45f0-89c3-bd875c7ae636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cd44e-9dcc-44bf-a3a2-2f4aa9dcce81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc92c59-434e-46d2-a84b-1db2b9aa44c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a03341-a4ed-4e18-b90d-243c63b24c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5791c-bb0b-445d-b095-4096d5d0b765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c2a0b-7c0c-4c6a-9e32-42daf2c48aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aea2c-e85c-4369-a641-1be74e9a62ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
