{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3731eed-0295-40d4-a02a-63a355012150",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852a70a-c384-40ea-9de2-8dd18f53284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet Regression is a regularized regression method that linearly combines the L1 and L2 penalties of the Lasso and Ridge methods. The algorithm’s primary goal is to minimize the complexity of the\n",
    "model by inducing the penalty against complexity. It does this by adding both a Ridge (L2) and Lasso (L1) penalty term to the loss function.\n",
    "\n",
    "1. Combines L1 and L2 Regularization: Elastic Net Regression combines the penalties of both Lasso and Ridge regression. Lasso tends to perform feature selection by shrinking the coefficients of less important features to zero, while Ridge tends to shrink the coefficients of correlated features towards each other. Elastic Net combines these two penalties, allowing for both feature selection and handling of correlated predictors.\n",
    "\n",
    "2. Addresses multicollinearity: Traditional regression techniques like Ordinary Least Squares (OLS) can struggle when features are highly correlated, leading to unstable and inaccurate coefficient estimates. Elastic Net Regression helps alleviate this issue by using both L1 and L2 penalties, which encourages sparse coefficients and accounts for correlated predictors.\n",
    "\n",
    "3. Tuning parameter: Elastic Net introduces an additional tuning parameter, α (alpha), which controls the balance between the L1 and L2 penalties. When α = 1, Elastic Net is equivalent to Lasso regression, and when α = 0, it's equivalent to Ridge regression. By adjusting α, practitioners can fine-tune the model's behavior to achieve the desired balance between sparsity and multicollinearity handling.\n",
    "\n",
    "4. Suitable for high-dimensional data: Elastic Net Regression is particularly useful when dealing with datasets containing a large number of predictors, where traditional regression techniques may overfit or perform poorly due to the curse of dimensionality. By incorporating both L1 and L2 penalties, Elastic Net can effectively handle high-dimensional data and prevent overfitting.\n",
    "\n",
    "5. Robustness: Elastic Net Regression tends to be more robust than individual regularization techniques like Lasso or Ridge regression alone. It inherits the advantages of both methods while mitigating their respective limitations. This makes Elastic Net a versatile choice for regression tasks in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6d904-3790-452c-9b51-4d9edcd56d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb1317-388e-4a60-bccd-b6273a6e0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Understand Elastic Net Regression: Elastic Net Regression combines L1 (Lasso) and L2 (Ridge) penalties to regularize the coefficients of the features. The regularization terms are controlled by two hyperparameters:\n",
    "   - Alpha (α): Controls the overall strength of the regularization. It's a mixture of L1 and L2 penalties.\n",
    "   - L1 Ratio (ρ): Determines the balance between L1 and L2 penalties. It's the ratio of L1 penalty in the regularization term to the total penalty.\n",
    "\n",
    "2. Define a Search Space: Decide on a range or set of values for both alpha and the L1 ratio that you want to search within. Typically, alphas range from 0 to 1, and the L1 ratio ranges from 0 to 1 as well.\n",
    "\n",
    "3. Choose a Cross-Validation Strategy: Cross-validation is crucial for hyperparameter tuning. Common techniques include k-fold cross-validation or train-validation split.\n",
    "\n",
    "4. Perform Grid Search or Random Search: There are two common approaches to search for the optimal hyperparameters:\n",
    "   - Grid Search: It exhaustively searches through all possible combinations of hyperparameters within the defined search space. While it guarantees finding the best combination, it can be computationally expensive.\n",
    "   - Random Search: It randomly samples combinations of hyperparameters within the search space. While it might not guarantee finding the absolute best combination, it's computationally cheaper and often discovers good combinations efficiently.\n",
    "\n",
    "5. Evaluate Models: For each combination of hyperparameters, fit the Elastic Net Regression model on the training data and evaluate its performance on the validation set using a suitable metric (e.g., mean squared error, mean absolute error, etc.).\n",
    "\n",
    "6. Select the Optimal Hyperparameters: Choose the combination of hyperparameters that gives the best performance on the validation set. This could be based on the lowest error or another relevant metric.\n",
    "\n",
    "7. Optional: Evaluate on Test Set: Once you've chosen the optimal hyperparameters, evaluate the model with those hyperparameters on a separate test set to get an unbiased estimate of its performance.\n",
    "\n",
    "8. Refinement (Optional): Sometimes, after initial tuning, it might be beneficial to refine the search space based on the results obtained and repeat the process to further fine-tune the hyperparameters.\n",
    "\n",
    "9. Final Model Training: Train the final Elastic Net Regression model using the optimal hyperparameters on the entire dataset (training + validation).\n",
    "\n",
    "10. Deploy the Model: Once you have the final trained model, you can deploy it for making predictions on new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792627ad-e7b0-4dc1-b000-fec6b3b8b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6efa3-0e84-4c77-a87d-45664b662e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "\n",
    "1. Handles multicollinearity: Like Ridge Regression, Elastic Net can handle multicollinearity well by penalizing the coefficients of correlated features. This helps to stabilize the model and prevent overfitting caused by high correlation between predictors.\n",
    "\n",
    "2. Feature selection: Similar to Lasso Regression, Elastic Net performs automatic feature selection by shrinking the coefficients of less important features towards zero. This can be particularly useful when dealing with high-dimensional datasets with many irrelevant or redundant features.\n",
    "\n",
    "3. Flexibility in controlling regularization: Elastic Net allows for greater flexibility in controlling the amount and type of regularization compared to Ridge or Lasso alone. This is achieved through the two hyperparameters: alpha (α) and the L1 ratio (ρ), providing more control over the trade-off between L1 and L2 penalties.\n",
    "\n",
    "4. More stable feature selection: Compared to Lasso Regression, which can be unstable when dealing with highly correlated features, Elastic Net tends to be more stable due to the presence of the Ridge penalty term. This can lead to more reliable feature selection results.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Complexity in hyperparameter tuning: Elastic Net Regression introduces two hyperparameters (alpha and the L1 ratio) that need to be tuned. This adds complexity to the model selection process and requires additional computational resources for hyperparameter optimization.\n",
    "\n",
    "2. Interpretability: While Elastic Net can perform feature selection, interpreting the coefficients of the selected features might be less straightforward compared to simpler models like ordinary least squares regression. This is because the coefficients are penalized and can be influenced by the regularization terms.\n",
    "\n",
    "3. Loss of sparsity: Although Elastic Net encourages sparsity by shrinking coefficients towards zero, it might not achieve as much sparsity as Lasso Regression alone. In cases where a high level of sparsity is desired, Lasso Regression might be preferred over Elastic Net.\n",
    "\n",
    "4. Computationally more expensive: Elastic Net Regression involves solving a more complex optimization problem compared to Ridge or Lasso Regression alone, as it combines both penalties. This can lead to increased computational cost, especially for large datasets or when using sophisticated optimization algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c508b-5f1a-4572-91a1-9fa80e9ce75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some common use cases for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cb0dd-83bd-42fa-a7c0-e67321afe171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. High-dimensional data: Elastic Net Regression is effective when dealing with datasets with a large number of predictors compared to the number of observations. This scenario is common in fields such as genomics, where the number of genes or genetic markers measured far exceeds the sample size.\n",
    "\n",
    "2. Multicollinearity: When predictors in a dataset are highly correlated, traditional regression models may exhibit instability or have difficulty in distinguishing the individual effects of predictors. Elastic Net Regression helps to address multicollinearity by simultaneously shrinking and selecting variables.\n",
    "\n",
    "3. Feature selection: Elastic Net Regression performs automatic feature selection by penalizing less important predictors and setting their coefficients to zero. This makes it valuable in scenarios where identifying the most relevant predictors is essential for model interpretability and performance.\n",
    "\n",
    "4. Predictive modeling: In predictive modeling tasks, Elastic Net Regression can be used to build robust models that generalize well to new data. It is commonly employed in areas such as finance for predicting stock prices or credit risk, in healthcare for predicting patient outcomes, and in marketing for customer segmentation and predictive analytics.\n",
    "\n",
    "5. Regularization: Elastic Net Regression is useful for regularization, which helps prevent overfitting by adding a penalty term to the regression objective function. By controlling the strength of regularization through the alpha parameter, Elastic Net allows for a balance between bias and variance, leading to more stable and generalizable models.\n",
    "\n",
    "6. Data mining and machine learning: Elastic Net Regression is widely used in machine learning tasks such as classification and regression, especially when dealing with noisy or high-dimensional data. It serves as a robust regression technique that can handle various types of data and modeling scenarios.\n",
    "\n",
    "7. Optimization problems: Elastic Net Regression can also be applied in optimization problems where the goal is to minimize a loss function subject to constraints. Its ability to handle regularization makes it suitable for optimization tasks with noisy or ill-conditioned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22b202-670e-4ae2-b4f1-dafecc78c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you interpret the coefficients in Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900350ee-0ced-487b-a68c-65e9805deb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Magnitude of Coefficients: The magnitude of a coefficient indicates the strength of the relationship between the corresponding predictor variable and the target variable. A larger coefficient magnitude suggests a stronger impact of that predictor on the target variable, all else being equal.\n",
    "\n",
    "2. Sign of Coefficients: The sign of a coefficient (+/-) indicates the direction of the relationship between the predictor variable and the target variable. A positive coefficient suggests a positive relationship (an increase in the predictor leads to an increase in the target), while a negative coefficient suggests a negative relationship (an increase in the predictor leads to a decrease in the target).\n",
    "\n",
    "3. Regularization Effect: In Elastic Net Regression, coefficients are subject to both L1 (Lasso) and L2 (Ridge) penalties, which can affect their magnitudes. \n",
    "   - Coefficients that survive the regularization process (i.e., not shrunk to zero) are considered significant predictors.\n",
    "   - The magnitude of coefficients is influenced by the balance between the L1 and L2 penalties, controlled by the alpha and L1 ratio hyperparameters. A higher alpha or higher L1 ratio tends to shrink coefficients more aggressively, potentially leading to more coefficients being set to zero.\n",
    "\n",
    "4. Comparison with Ordinary Least Squares (OLS) Regression: In ordinary least squares regression, coefficients are estimated without regularization, and each coefficient represents the change in the target variable for a one-unit change in the corresponding predictor, holding all other predictors constant. However, in Elastic Net Regression, coefficients are adjusted to account for the regularization penalties, so their interpretation should consider the regularization effect.\n",
    "\n",
    "5. Relative Importance: Comparing the magnitudes of coefficients can provide insights into the relative importance of different predictors in explaining the variability of the target variable. However, caution should be exercised in interpreting coefficients as measures of importance, especially in the presence of multicollinearity or when predictors are on different scales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aa1c1-8f19-4cf9-afca-3d6c619b8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values when using Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210c8b0-61f4-4a25-9982-20452a2f31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Imputation:\n",
    "   - Impute missing values by replacing them with estimated values based on other observations.\n",
    "   - Common imputation methods include mean imputation, median imputation, or regression imputation.\n",
    "   - Use libraries like scikit-learn's `SimpleImputer` to perform imputation.\n",
    "\n",
    "2. Remove Missing Data:\n",
    "   - Remove rows with missing values from the dataset.\n",
    "   - Be cautious when using this approach, as it reduces the available data for training.\n",
    "\n",
    "3. Feature Engineering:\n",
    "   - Create new features that capture information related to missingness.\n",
    "   - For example, add a binary indicator variable that represents whether a value is missing for a specific feature.\n",
    "\n",
    "4. Model-Based Imputation:\n",
    "   - Use other features to predict missing values.\n",
    "   - Train a separate model (e.g., linear regression) to predict the missing values based on available features.\n",
    "\n",
    "5. Elastic Net and Missing Data:\n",
    "   - Elastic Net itself does not handle missing values directly.\n",
    "   - Preprocess the data by imputing missing values before applying Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe0d3f-af9e-46a2-8f69-e5ce1265e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do you use Elastic Net Regression for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2267f-c718-4f7d-9924-f201aff9ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Regularization Strength Parameters:\n",
    "   - Elastic Net introduces two hyperparameters:\n",
    "     - \\(\\lambda_1\\) (L1 penalty): Controls the strength of lasso regularization.\n",
    "     - \\(\\lambda_2\\) (L2 penalty): Controls the strength of ridge regularization.\n",
    "   - These parameters determine the trade-off between feature selection and coefficient shrinkage.\n",
    "\n",
    "2. Feature Selection Mechanism:\n",
    "   - Elastic Net balances the effects of ridge and lasso:\n",
    "     - Lasso encourages sparsity by setting some coefficients to zero.\n",
    "     - Ridge encourages small but non-zero coefficients.\n",
    "   - The combination of these penalties allows Elastic Net to select relevant features while shrinking others.\n",
    "\n",
    "3. Steps for Feature Selection with Elastic Net:\n",
    "   - Data Preparation:\n",
    "     - Handle missing values and preprocess the data.\n",
    "   - Feature Selection:\n",
    "     - Select a subset of relevant features (e.g., based on domain knowledge or exploratory data analysis).\n",
    "   - Hyperparameter Tuning:\n",
    "     - Use cross-validation to find optimal values for \\(\\lambda_1\\) and \\(\\lambda_2\\).\n",
    "   - Train Elastic Net Model:\n",
    "     - Fit the Elastic Net model using the selected features and tuned hyperparameters.\n",
    "   - Coefficient Interpretation:\n",
    "     - Examine the coefficients:\n",
    "       - Non-zero coefficients correspond to relevant features.\n",
    "       - Zero coefficients correspond to excluded features.\n",
    "\n",
    "4. Benefits of Elastic Net for Feature Selection:\n",
    "   - Robustness: Handles multicollinearity and outliers.\n",
    "   - Flexibility: Allows you to control the trade-off between ridge and lasso.\n",
    "   - Sparse Solutions: Produces sparse models with relevant features.\n",
    "\n",
    "5. Context Matter*:\n",
    "   - Consider the problem context, data characteristics, and business goals.\n",
    "   - Experiment with different \\(\\lambda_1\\) and \\(\\lambda_2\\) values to find the right balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b7ff9-7ec5-4c28-b3d7-3228da1125ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "elastic_net.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7e35d-0184-43e1-a807-5ea63d5944ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4b0987-968f-448a-bb27-0c5594c82903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 133.25851014728863\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "with open(\"elastic_net_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(elastic_net, f)\n",
    "\n",
    "with open(\"elastic_net_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "y_pred = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03585d7e-b40b-4788-baec-419a21d34c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the purpose of pickling a model in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f30053-b5cb-480b-90c8-da2463e4e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Reuse: Once a machine learning model has been trained on a dataset, pickling allows you to save the model object to disk. This enables you to reuse the trained model without needing to retrain it every time you need to make predictions.\n",
    "\n",
    "2. Scalability: For large datasets or complex models that require significant computational resources to train, pickling can help save time and resources by allowing you to train the model once and then reuse it multiple times.\n",
    "\n",
    "3. Deployment: Pickling is commonly used in deploying machine learning models in production environments. Once a model has been trained and validated, it can be pickled and deployed to serve predictions to end-users or integrate into applications.\n",
    "\n",
    "4. Sharing: Pickling allows you to easily share trained models with collaborators or other stakeholders. By saving the model to a file, you can transfer it to other machines or share it over a network for further analysis or deployment.\n",
    "\n",
    "5. Versioning: Pickling enables you to version control trained models along with your codebase. You can save multiple versions of a trained model and track changes over time, making it easier to reproduce experiments and track model performance.\n",
    "\n",
    "6. State Preservation: Pickling not only saves the model parameters but also preserves the internal state of the model, including the training data, hyperparameters, and any other attributes associated with the model object. This ensures that the model can be restored to its exact state when it was pickled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80096366-b62d-4b5f-a328-cda647f29985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330f11f-e551-451c-b5ed-286d2973177c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd3365-7471-4f25-8364-2567b56cdd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5dc4c-161d-43d2-b72a-a6a78c157cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3eede-a997-44ed-b984-47c99e8ab286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515e958-8103-43b7-b82f-04a97b039690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b27e70-ea28-4e3a-9827-7e500b996c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c01a5-23e4-43a3-b78e-03248658e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb240b27-cddc-4460-a4e0-55f30c73dea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4bf53-a1e2-4c23-9c38-8cc42d48a8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526087a5-da8c-4729-a8e8-4a4b6d0fcfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4898e-870a-4db4-a1e0-cb0518e7b075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aab9f9-712f-4635-81ae-2dbb440ad338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f187-90ea-4454-940b-e600403f9c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b972a33-151c-4e7f-9228-01f5bf16c44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2c051-f36a-45a3-833e-78aa2415d20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e61f4-76b9-4fb4-b1ee-183073af7d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
